{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Skill Gap Analysis - Graph Exploration\n",
        "\n",
        "Este notebook contiene análisis exploratorio de grafos de habilidades y experimentos con clustering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add parent directory to path\n",
        "sys.path.append(str(Path().resolve().parent))\n",
        "\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "from collections import Counter\n",
        "\n",
        "from core.graph_analysis import (\n",
        "    build_bipartite_graph,\n",
        "    build_skill_cooccurrence_graph,\n",
        "    compute_centralities,\n",
        "    detect_communities\n",
        ")\n",
        "from core.analysis import cluster_jobs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Cargar Datos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed jobs data\n",
        "data_files = [\n",
        "    \"../data/processed_jobs_data_analyst_madrid.csv\",\n",
        "    \"../data/processed_jobs_data_analyst_barcelona.csv\",\n",
        "    \"../data/processed_jobs_data_scientist_barcelona.csv\"\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "for file in data_files:\n",
        "    try:\n",
        "        df = pd.read_csv(file)\n",
        "        # Convert skills_detected from string to list if needed\n",
        "        if \"skills_detected\" in df.columns:\n",
        "            df[\"skills_detected\"] = df[\"skills_detected\"].apply(\n",
        "                lambda x: [s.strip() for s in str(x).split(\",\") if s.strip()] if pd.notna(x) and x else []\n",
        "            )\n",
        "        dfs.append(df)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file}\")\n",
        "\n",
        "if dfs:\n",
        "    jobs_df = pd.concat(dfs, ignore_index=True)\n",
        "    print(f\"Loaded {len(jobs_df)} jobs\")\n",
        "    print(f\"Columns: {jobs_df.columns.tolist()}\")\n",
        "else:\n",
        "    print(\"No data files found. Please run the main app first to generate data.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Análisis de Grafos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build skill co-occurrence graph\n",
        "skill_graph = build_skill_cooccurrence_graph(jobs_df)\n",
        "\n",
        "print(f\"Number of nodes (skills): {skill_graph.number_of_nodes()}\")\n",
        "print(f\"Number of edges: {skill_graph.number_of_edges()}\")\n",
        "print(f\"Graph density: {nx.density(skill_graph):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute centralities\n",
        "centralities_df = compute_centralities(skill_graph)\n",
        "print(\"Top 10 skills by degree centrality:\")\n",
        "print(centralities_df.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect communities\n",
        "communities = detect_communities(skill_graph)\n",
        "print(f\"Number of communities: {len(set(communities.values()))}\")\n",
        "\n",
        "# Show skills by community\n",
        "from collections import defaultdict\n",
        "comm_dict = defaultdict(list)\n",
        "for skill, comm_id in communities.items():\n",
        "    comm_dict[comm_id].append(skill)\n",
        "\n",
        "for comm_id, skills in sorted(comm_dict.items()):\n",
        "    print(f\"\\nCommunity {comm_id} ({len(skills)} skills):\")\n",
        "    print(\", \".join(sorted(skills)[:10]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Clustering de Ofertas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply clustering\n",
        "if \"match_ratio\" not in jobs_df.columns:\n",
        "    # Add dummy match_ratio if not present\n",
        "    jobs_df[\"match_ratio\"] = 0.5\n",
        "\n",
        "clustered_df = cluster_jobs(jobs_df, n_clusters=4)\n",
        "\n",
        "print(\"Cluster distribution:\")\n",
        "print(clustered_df[\"cluster\"].value_counts().sort_index())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze clusters\n",
        "from core.analysis import interpret_clusters\n",
        "\n",
        "cluster_summary = interpret_clusters(clustered_df)\n",
        "print(\"Cluster interpretations:\")\n",
        "print(cluster_summary)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
